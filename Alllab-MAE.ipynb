{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1aa2430-0c5b-4aa2-bd36-8727e08e7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'sagemaker-studio-905418013525-nvxe84zgs6'\n",
    "file_path = 'Labrado/alllabs1000adm/Labitemlist.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "Labitemlist = pd.read_csv(obj['Body'], header=None,names=['labitem', 'cnt'], dtype=int)\n",
    "\n",
    "file_path = 'Labrado/alllabs1000adm/unique_adm_year_day.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "unique_adm_year_day = pd.read_csv(obj['Body'], header=None,names=['hadm_id', 'chartyear', 'day','rowinx'], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "79a3aaa4-1ac5-4089-876b-28342cacf8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labitemlist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1a942a1-3beb-4c0f-ad93-63c621d60247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>chartyear</th>\n",
       "      <th>day</th>\n",
       "      <th>rowinx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000019</td>\n",
       "      <td>2159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000019</td>\n",
       "      <td>2159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000019</td>\n",
       "      <td>2159</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20000024</td>\n",
       "      <td>2151</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20000034</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hadm_id  chartyear  day  rowinx\n",
       "0  20000019       2159    0       0\n",
       "1  20000019       2159    1       1\n",
       "2  20000019       2159    2       2\n",
       "3  20000024       2151    0       3\n",
       "4  20000034       2174    0       4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_adm_year_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e681e8f-7869-4fc4-9198-9175a4615fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split year is 2178\n"
     ]
    }
   ],
   "source": [
    "unique_hadm_by_year = unique_adm_year_day.groupby('chartyear')['hadm_id'].nunique()\n",
    "cumulative_sum = unique_hadm_by_year.cumsum()\n",
    "total_sum = cumulative_sum.iloc[-1]\n",
    "split_index = cumulative_sum[cumulative_sum >= 0.8 * total_sum].index[0]\n",
    "print(\"split year is\",split_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a471c15c-d0fb-44a4-986c-89b57e2f0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "Row_Train = unique_adm_year_day['chartyear']<=split_index\n",
    "Train_inx = Row_Train[Row_Train].index\n",
    "Row_Test = unique_adm_year_day['chartyear']>split_index\n",
    "Test_inx = Row_Test[Row_Test].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44126e9a-2fc3-47a4-8e1c-259cc43213f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array = np.genfromtxt('filename.txt', delimiter=' ', usecols=range(10))-- to test later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "38fa470a-eee2-484f-a32d-9a7f2733e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Labrado/alllabs1000adm/npval.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "npval = pd.read_csv(obj['Body'], header=None, dtype=float)\n",
    "npval_train = npval.iloc[Train_inx,:100]\n",
    "npval_test = npval.iloc[Test_inx,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8c49b9de-06fb-4e86-bd12-d0ba9fdc8c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "# current implementation: only support numerical values\n",
    "import numpy as np\n",
    "import torch, os\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "class MaskEmbed(nn.Module):\n",
    "    \"\"\" record to mask embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, rec_len=25, embed_dim=64, norm_layer=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.rec_len = rec_len\n",
    "        self.proj = nn.Conv1d(1, embed_dim, kernel_size=1, stride=1)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L = x.shape\n",
    "        # assert(L == self.rec_len, f\"Input data width ({L}) doesn't match model ({self.rec_len}).\")\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ActiveEmbed(nn.Module):\n",
    "    \"\"\" record to mask embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, rec_len=25, embed_dim=64, norm_layer=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.rec_len = rec_len\n",
    "        self.proj = nn.Conv1d(1, embed_dim, kernel_size=1, stride=1)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L = x.shape\n",
    "        # assert(L == self.rec_len, f\"Input data width ({L}) doesn't match model ({self.rec_len}).\")\n",
    "        x = self.proj(x)\n",
    "        x = torch.sin(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        #   x = torch.cat((torch.sin(x), torch.cos(x + math.pi/2)), -1)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, pos, cls_token=False):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = np.arange(pos)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    pos_embed = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr, min_lr, max_epochs, warmup_epochs):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        tmp_lr = lr * epoch / warmup_epochs \n",
    "    else:\n",
    "        tmp_lr = min_lr + (lr - min_lr) * 0.5 * \\\n",
    "            (1. + math.cos(math.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = tmp_lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = tmp_lr\n",
    "    return tmp_lr\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == np.inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "class NativeScaler:\n",
    "\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "class MAEDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, M):        \n",
    "         self.X = X\n",
    "         self.M = M\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.X[idx], self.M[idx]\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(dataset : str, path : str):\n",
    "\n",
    "    if dataset in ['climate', 'compression', 'wine', 'yacht', 'spam', 'letter', 'credit', 'raisin', 'bike', 'obesity', 'airfoil', 'blood', 'yeast', 'health', 'review', 'travel']:\n",
    "        df = pd.read_csv(os.path.join(path, 'data', dataset + '.csv'))\n",
    "        last_col = df.columns[-1]\n",
    "        y = df[last_col]\n",
    "        X = df.drop(columns=[last_col])\n",
    "    elif dataset == 'california':\n",
    "        from sklearn.datasets import fetch_california_housing\n",
    "        X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "    elif dataset == 'diabetes':\n",
    "        from sklearn.datasets import load_diabetes\n",
    "        X, y = load_diabetes(as_frame=True, return_X_y=True)\n",
    "    elif dataset == 'iris':\n",
    "        # only for testing\n",
    "        from sklearn.datasets import load_iris\n",
    "        X, y = load_iris(as_frame=True, return_X_y=True)\n",
    "        print(\"loaded iris dataset\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('MAE pre-training', add_help=False)\n",
    "    parser.add_argument('--dataset', default='california', type=str)\n",
    "    parser.add_argument('--batch_size', default=64, type=int,\n",
    "                        help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus')\n",
    "    parser.add_argument('--max_epochs', default=600, type=int)\n",
    "    parser.add_argument('--accum_iter', default=1, type=int,\n",
    "                        help='Accumulate gradient iterations (for increasing the effective batch size under memory constraints)')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--mask_ratio', default=0.5, type=float, help='Masking ratio (percentage of removed patches).')\n",
    "    parser.add_argument('--embed_dim', default=32, type=int, help='embedding dimensions')\n",
    "    parser.add_argument('--depth', default=6, type=int, help='encoder depth')\n",
    "    parser.add_argument('--decoder_depth', default=4, type=int, help='decoder depth')\n",
    "    parser.add_argument('--num_heads', default=4, type=int, help='number of heads')\n",
    "    parser.add_argument('--mlp_ratio', default=4., type=float, help='mlp ratio')\n",
    "    parser.add_argument('--encode_func', default='linear', type=str, help='encoding function')\n",
    "\n",
    "    parser.add_argument('--norm_field_loss', default=False,\n",
    "                        help='Use (per-patch) normalized field as targets for computing loss')\n",
    "    parser.set_defaults(norm_field_loss=False)\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05, help='weight decay (default: 0.05)')\n",
    "    parser.add_argument('--lr', type=float, default=None, metavar='LR', help='learning rate (absolute lr)')\n",
    "    parser.add_argument('--blr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='base learning rate: absolute_lr = base_lr * total_batch_size / 256')\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-5, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0')\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=40, metavar='N', help='epochs to warmup LR')\n",
    "\n",
    "    ###### change this path\n",
    "    parser.add_argument('--path', default='/data/tianyu/remasker/', type=str, help='dataset path')\n",
    "    parser.add_argument('--exp_name', default='test', type=str, help='experiment name')\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--device', default='cuda', help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=666, type=int)\n",
    "\n",
    "    parser.add_argument('--overwrite', default=True, help='whether to overwrite default config')\n",
    "    parser.add_argument('--pin_mem', action='store_false')\n",
    "\n",
    "    # distributed training parameters\n",
    "    return parser\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    X = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n",
    "    X = X.unsqueeze(1)\n",
    "    mask_embed = ActiveEmbed(4, 6)\n",
    "    print(mask_embed(X).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71731f1b-1203-42cb-9d47-5d3a87dabb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.5094, grad_fn=<AddBackward0>), tensor([[[0.8098],\n",
      "         [0.8507],\n",
      "         [0.1312],\n",
      "         [0.1784]]], grad_fn=<SliceBackward0>), tensor([[0., 0., 0., 1.]]), tensor([[0., 1., 0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "# current implementation: only support numerical values\n",
    "\n",
    "from functools import partial\n",
    "from tkinter import E\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from timm.models.vision_transformer import Block\n",
    "from utils import MaskEmbed, get_1d_sincos_pos_embed, ActiveEmbed\n",
    "eps = 1e-6\n",
    "\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Masked Autoencoder with Transformer backbone\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rec_len=25, embed_dim=64, depth=4, num_heads=4,\n",
    "        decoder_embed_dim=64, decoder_depth=2, decoder_num_heads=4,\n",
    "        mlp_ratio=4., norm_layer=nn.LayerNorm, norm_field_loss=False, encode_func='linear'):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        \n",
    "        if encode_func == 'active':\n",
    "            self.mask_embed = ActiveEmbed(rec_len, embed_dim)\n",
    "        else:\n",
    "            self.mask_embed = MaskEmbed(rec_len, embed_dim)\n",
    "        \n",
    "        self.rec_len = rec_len\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, rec_len + 1, embed_dim), requires_grad=False)  \n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, rec_len + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, 1, bias=True)  # decoder to patch\n",
    "        \n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_field_loss = norm_field_loss\n",
    "        self.initialize_weights()\n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \n",
    "        # initialization\n",
    "        \n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.mask_embed.rec_len, cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_1d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], self.mask_embed.rec_len, cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.mask_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "    def random_masking(self, x, m, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        if self.training:\n",
    "            len_keep = int(L * (1 - mask_ratio))\n",
    "        else:\n",
    "            len_keep = int(torch.min(torch.sum(m, dim=1)))\n",
    "\n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        noise[m < eps] = 1\n",
    "\n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "        \n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "        nask = torch.ones([N, L], device=x.device) - mask\n",
    "\n",
    "        if self.training:\n",
    "            mask[m < eps] = 0\n",
    "\n",
    "        return x_masked, mask, nask, ids_restore\n",
    "\n",
    "\n",
    "    def forward_encoder(self, x, m, mask_ratio=0.5):\n",
    "        \n",
    "        # embed patches\n",
    "        x = self.mask_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]    \n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, nask, ids_restore = self.random_masking(x, m, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, nask, ids_restore\n",
    "\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        \n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        # x = self.decoder_pred(x)\n",
    "        x = torch.tanh(self.decoder_pred(x))/2 + 0.5\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward_loss(self, data, pred, mask, nask):\n",
    "        \"\"\"\n",
    "        data: [N, 1, L]\n",
    "        pred: [N, L]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        # target = self.patchify(data)\n",
    "        target = data.squeeze(dim=1)\n",
    "        if self.norm_field_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + eps)**.5\n",
    "        \n",
    "        \n",
    "        loss = (pred.squeeze(dim=2) - target) ** 2\n",
    "        loss = (loss * mask).sum() / mask.sum()  + (loss * nask).sum() / nask.sum()\n",
    "        # mean loss on removed patches\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def forward(self, data, miss_idx, mask_ratio=0.5):\n",
    "        \n",
    "        latent, mask, nask, ids_restore = self.forward_encoder(data, miss_idx, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore) \n",
    "        loss = self.forward_loss(data, pred, mask, nask)\n",
    "        return loss, pred, mask, nask\n",
    "\n",
    "\n",
    "def mae_base(**kwargs):\n",
    "    model = MaskedAutoencoder(\n",
    "        embed_dim=64, depth=8, num_heads=4,\n",
    "        decoder_embed_dim=64, decoder_depth=4, decoder_num_heads=4,\n",
    "        mlp_ratio=2., norm_layer=partial(nn.LayerNorm, eps=eps), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_medium(**kwargs):\n",
    "    model = MaskedAutoencoder(\n",
    "        embed_dim=32, depth=4, num_heads=4,\n",
    "        decoder_embed_dim=32, decoder_depth=4, decoder_num_heads=4,\n",
    "        mlp_ratio=4., norm_layer=partial(nn.LayerNorm, eps=eps), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_large(**kwargs):\n",
    "    model = MaskedAutoencoder(\n",
    "        embed_dim=64, depth=8, num_heads=4,\n",
    "        decoder_embed_dim=64, decoder_depth=4, decoder_num_heads=4,\n",
    "        mlp_ratio=4., norm_layer=partial(nn.LayerNorm, eps=eps), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    model = MaskedAutoencoder(\n",
    "        rec_len=4, embed_dim=8, depth=1, num_heads=1,\n",
    "        decoder_embed_dim=8, decoder_depth=1, decoder_num_heads=1,\n",
    "        mlp_ratio=4., norm_layer=partial(nn.LayerNorm, eps=eps)\n",
    "    )\n",
    "    \n",
    "    X = pd.DataFrame([[np.nan, 0.5, np.nan, 0.8]])\n",
    "\n",
    "    X = torch.tensor(X.values, dtype=torch.float32)\n",
    "    M = 1 - (1 * (np.isnan(X)))\n",
    "    X = torch.nan_to_num(X)\n",
    "    \n",
    "    X = X.unsqueeze(dim=1)\n",
    "    print(model.forward(X, M, 0.75))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a25f6c85-1e6d-462a-ba17-0ada239db529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from typing import Any, List, Tuple, Union\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import math, sys, argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "import time, os, json\n",
    "from utils import NativeScaler, MAEDataset, adjust_learning_rate, get_dataset\n",
    "import model_mae\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import sys\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "from utils import get_args_parser\n",
    "\n",
    "# hyperimpute absolute\n",
    "from hyperimpute.plugins.imputers import ImputerPlugin\n",
    "from sklearn.datasets import load_iris\n",
    "from hyperimpute.utils.benchmarks import compare_models\n",
    "from hyperimpute.plugins.imputers import Imputers\n",
    "from tqdm import tqdm\n",
    "eps = 1e-8\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ReMasker:\n",
    "\n",
    "    def __init__(self):\n",
    "        args = get_args_parser().parse_args()\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "        self.accum_iter = args.accum_iter\n",
    "        self.min_lr = args.min_lr\n",
    "        self.norm_field_loss = args.norm_field_loss\n",
    "        self.weight_decay = args.weight_decay\n",
    "        self.lr = args.lr\n",
    "        self.blr = args.blr\n",
    "        self.warmup_epochs = 20\n",
    "        self.model = None\n",
    "        self.norm_parameters = None\n",
    "\n",
    "        self.embed_dim = args.embed_dim\n",
    "        self.depth = args.depth\n",
    "        self.decoder_depth = args.decoder_depth\n",
    "        self.num_heads = args.num_heads\n",
    "        self.mlp_ratio = args.mlp_ratio\n",
    "        self.max_epochs = 50\n",
    "        self.mask_ratio = 0.5\n",
    "        self.encode_func = args.encode_func\n",
    "\n",
    "    def fit(self, X_raw: pd.DataFrame):\n",
    "\n",
    "        X = X_raw.copy()\n",
    "\n",
    "        # Parameters\n",
    "        no = len(X)\n",
    "        dim = X.shape[1]\n",
    "\n",
    "        # X = X.cpu()\n",
    "\n",
    "        min_val = np.zeros(dim)\n",
    "        max_val = np.zeros(dim)\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# Assuming X is a DataFrame and dim is the number of columns\n",
    "        dim = X.shape[1]\n",
    "        min_val = np.zeros(dim)\n",
    "        max_val = np.zeros(dim)\n",
    "        eps = 1e-7\n",
    "\n",
    "        for i in range(dim):\n",
    "            # Use .iloc to access the DataFrame by integer-location\n",
    "            min_val[i] = np.nanmin(X.iloc[:, i])\n",
    "            max_val[i] = np.nanmax(X.iloc[:, i])\n",
    "            # Perform the operation and update the column\n",
    "            X.iloc[:, i] = (X.iloc[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n",
    "\n",
    "        self.norm_parameters = {\"min\": min_val, \"max\": max_val}\n",
    "        np_array = X.to_numpy()\n",
    "\n",
    "        # Convert NumPy array to PyTorch tensor\n",
    "        X = torch.tensor(np_array, dtype=torch.float32)\n",
    "        # Set missing\n",
    "        M = 1 - (1 * (np.isnan(X)))\n",
    "        M = M.float().to(device)\n",
    "\n",
    "        X = torch.nan_to_num(X)\n",
    "        X = X.to(device)\n",
    "\n",
    "        self.model = model_mae.MaskedAutoencoder(\n",
    "            rec_len=dim,\n",
    "            embed_dim=self.embed_dim,\n",
    "            depth=self.depth,\n",
    "            num_heads=self.num_heads,\n",
    "            decoder_embed_dim=self.embed_dim,\n",
    "            decoder_depth=self.decoder_depth,\n",
    "            decoder_num_heads=self.num_heads,\n",
    "            mlp_ratio=self.mlp_ratio,\n",
    "            norm_layer=partial(nn.LayerNorm, eps=eps),\n",
    "            norm_field_loss=self.norm_field_loss,\n",
    "            encode_func=self.encode_func\n",
    "        )\n",
    "\n",
    "        # if self.improve and os.path.exists(self.path):\n",
    "        #     self.model.load_state_dict(torch.load(self.path))\n",
    "        #     self.model.to(device)\n",
    "        #     return self\n",
    "\n",
    "        self.model.to(device)\n",
    "\n",
    "        # set optimizers\n",
    "        # param_groups = optim_factory.add_weight_decay(model, args.weight_decay)\n",
    "        eff_batch_size = self.batch_size * self.accum_iter\n",
    "        if self.lr is None:  # only base_lr is specified\n",
    "            self.lr = self.blr * eff_batch_size / 64\n",
    "        # param_groups = optim_factory.add_weight_decay(self.model, self.weight_decay)\n",
    "        # optimizer = torch.optim.AdamW(param_groups, lr=self.lr, betas=(0.9, 0.95))\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, betas=(0.9, 0.95))\n",
    "        loss_scaler = NativeScaler()\n",
    "\n",
    "        dataset = MAEDataset(X, M)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, sampler=RandomSampler(dataset),\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        # if self.resume and os.path.exists(self.path):\n",
    "        #     self.model.load_state_dict(torch.load(self.path))\n",
    "        #     self.lr *= 0.5\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(self.max_epochs):\n",
    "            print(epoch)\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = 0\n",
    "\n",
    "            iter = 0\n",
    "\n",
    "            for iter, (samples, masks) in tqdm(enumerate(dataloader), total = len(dataloader)):\n",
    "\n",
    "                # we use a per iteration (instead of per epoch) lr scheduler\n",
    "                if iter % self.accum_iter == 0:\n",
    "                    adjust_learning_rate(optimizer, iter / len(dataloader) + epoch, self.lr, self.min_lr,\n",
    "                                         self.max_epochs, self.warmup_epochs)\n",
    "\n",
    "                samples = samples.unsqueeze(dim=1)\n",
    "                samples = samples.to(device, non_blocking=True)\n",
    "                masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "                # print(samples, masks)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss, _, _, _ = self.model(samples, masks, mask_ratio=self.mask_ratio)\n",
    "                    loss_value = loss.item()\n",
    "                    total_loss += loss_value\n",
    "\n",
    "                if not math.isfinite(loss_value):\n",
    "                    print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "                    sys.exit(1)\n",
    "\n",
    "                loss /= self.accum_iter\n",
    "                loss_scaler(loss, optimizer, parameters=self.model.parameters(),\n",
    "                            update_grad=(iter + 1) % self.accum_iter == 0)\n",
    "\n",
    "                if (iter + 1) % self.accum_iter == 0:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            total_loss = (total_loss / (iter + 1)) ** 0.5\n",
    "            # if total_loss < best_loss:\n",
    "            #     best_loss = total_loss\n",
    "            #     torch.save(self.model.state_dict(), self.path)\n",
    "            # if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            # print((epoch+1),',', total_loss)\n",
    "\n",
    "        # torch.save(self.model.state_dict(), self.path)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_raw: torch.Tensor):\n",
    "        if not torch.is_tensor(X_raw):\n",
    "            X_raw = torch.tensor(X_raw.values) \n",
    "        X = X_raw.clone()\n",
    "\n",
    "        min_val = self.norm_parameters[\"min\"]\n",
    "        max_val = self.norm_parameters[\"max\"]\n",
    "\n",
    "        no, dim = X.shape\n",
    "        X = X.cpu()\n",
    "\n",
    "        # MinMaxScaler normalization\n",
    "        for i in range(dim):\n",
    "            X[:, i] = (X[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n",
    "\n",
    "        # Set missing\n",
    "        M = 1 - (1 * (np.isnan(X)))\n",
    "        X = np.nan_to_num(X)\n",
    "\n",
    "        X = torch.from_numpy(X).to(device).float()\n",
    "        M = M.to(device).float()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Imputed data\n",
    "        with torch.no_grad():\n",
    "            for i in range(no):\n",
    "                sample = torch.reshape(X[i], (1, 1, -1))\n",
    "                mask = torch.reshape(M[i], (1, -1))\n",
    "                _, pred, _, _ = self.model(sample, mask)\n",
    "                pred = pred.squeeze(dim=2)\n",
    "                if i == 0:\n",
    "                    imputed_data = pred\n",
    "                else:\n",
    "                    imputed_data = torch.cat((imputed_data, pred), 0)\n",
    "\n",
    "                    # Renormalize\n",
    "        for i in range(dim):\n",
    "            imputed_data[:, i] = imputed_data[:, i] * (max_val[i] - min_val[i] + eps) + min_val[i]\n",
    "\n",
    "        if np.all(np.isnan(imputed_data.detach().cpu().numpy())):\n",
    "            err = \"The imputed result contains nan. This is a bug. Please report it on the issue tracker.\"\n",
    "            raise RuntimeError(err)\n",
    "\n",
    "        M = M.cpu()\n",
    "        imputed_data = imputed_data.detach().cpu()\n",
    "        # print('imputed', imputed_data, M)\n",
    "        # print('imputed', M * np.nan_to_num(X_raw.cpu()) + (1 - M) * imputed_data)\n",
    "        return M * np.nan_to_num(X_raw.cpu()) + (1 - M) * imputed_data\n",
    "\n",
    "    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Imputes the provided dataset using the GAIN strategy.\n",
    "        Args:\n",
    "            X: np.ndarray\n",
    "                A dataset with missing values.\n",
    "        Returns:\n",
    "            Xhat: The imputed dataset.\n",
    "        \"\"\"\n",
    "        X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        return self.fit(X).transform(X).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5340f177-2bcf-4bc8-83e6-fcbd0a1afa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import get_args_parser\n",
    "from remasker_impute import ReMasker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "# X_raw = np.arange(50).reshape(10, 5) * 1.0\n",
    "# X = pd.DataFrame(X_raw, columns=['0', '1', '2', '3', '4'])\n",
    "# X.iat[3,0] = np.nan\n",
    "\n",
    "# imputer = ReMasker()\n",
    "# print(X)\n",
    "\n",
    "# imputed = imputer.fit_transform(X)\n",
    "# print(imputed)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7052a65-6f90-4dba-9a9f-3b49f252ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'sagemaker-studio-905418013525-nvxe84zgs6'\n",
    "file_path = 'Labrado/alllabs1000adm/alllab1000adm.txt'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "rawdata = obj[\"Body\"].read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a9840d5-6b60-46e8-9bfa-dd05a3b9de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(sublist) for sublist in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1aed44cb-a2f6-4a6b-9f79-85d80a7241a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>434243.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>450.466776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>896.759713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>206.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>479.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48506.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "count  434243.000000\n",
       "mean      450.466776\n",
       "std       896.759713\n",
       "min         0.000000\n",
       "25%        80.000000\n",
       "50%       206.000000\n",
       "75%       479.000000\n",
       "max     48506.000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f08931e1-e0c5-4e94-b374-6a36f0faf03b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m data_lines:\n\u001b[1;32m      5\u001b[0m     words \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()  \u001b[38;5;66;03m# Splits by any whitespace\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     year \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m year\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2178\u001b[39m:\n\u001b[1;32m      8\u001b[0m         train\u001b[38;5;241m.\u001b[39mappend(words[\u001b[38;5;241m2\u001b[39m:])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data_lines = rawdata.split('\\n')\n",
    "train = []\n",
    "test = []\n",
    "# Iterate through each line and split by spaces\n",
    "for line in data_lines:\n",
    "    words = line.split()  # Splits by any whitespace\n",
    "    year = int(words[1])\n",
    "    if len(words)>0:\n",
    "        if year<=2178:\n",
    "            train.append(words[2:])\n",
    "        else:\n",
    "            test.append(words[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4dc139af-6939-4c49-b5ff-dc2743af56eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: MAE pre-training [--dataset DATASET] [--batch_size BATCH_SIZE]\n",
      "                        [--max_epochs MAX_EPOCHS] [--accum_iter ACCUM_ITER]\n",
      "                        [--mask_ratio MASK_RATIO] [--embed_dim EMBED_DIM]\n",
      "                        [--depth DEPTH] [--decoder_depth DECODER_DEPTH]\n",
      "                        [--num_heads NUM_HEADS] [--mlp_ratio MLP_RATIO]\n",
      "                        [--encode_func ENCODE_FUNC]\n",
      "                        [--norm_field_loss NORM_FIELD_LOSS]\n",
      "                        [--weight_decay WEIGHT_DECAY] [--lr LR] [--blr LR]\n",
      "                        [--min_lr LR] [--warmup_epochs N] [--path PATH]\n",
      "                        [--exp_name EXP_NAME] [--device DEVICE] [--seed SEED]\n",
      "                        [--overwrite OVERWRITE] [--pin_mem]\n",
      "MAE pre-training: error: unrecognized arguments: -f /home/sagemaker-user/.local/share/jupyter/runtime/kernel-d21f0679-d5a2-4802-a731-046601ab1110.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "X_test = npval_test\n",
    "X_train = npval_train\n",
    "\n",
    "\n",
    "\n",
    "# # Function to generate new column names\n",
    "# def generate_new_name(old_name):\n",
    "#     counter=1\n",
    "#     mapping = {}\n",
    "#     # Check if the column name is numeric or has a \"_yesterday\" suffix\n",
    "#     base_name = old_name.replace('_yesterday', '')\n",
    "#     if base_name.isdigit():\n",
    "#         if base_name not in mapping:\n",
    "#             # Assign a new testX name and increment the counter\n",
    "#             mapping[base_name] = f'test{counter}'\n",
    "#             counter += 1\n",
    "#         # Return the mapped name, with \"_yesterday\" suffix if necessary\n",
    "#         return mapping[base_name] + ('_yesterday' if '_yesterday' in old_name else '')\n",
    "#     else:\n",
    "#         # Non-numeric columns remain unchanged\n",
    "#         return old_name\n",
    "\n",
    "# # Apply the renaming function to each column\n",
    "# test.columns = [generate_new_name(col) for col in test.columns]\n",
    "# X_test = test.filter(regex='^test[1-8](_yesterday)?$') \n",
    "\n",
    "# train.columns = [generate_new_name(col) for col in train.columns]\n",
    "# X_train = train.filter(regex='^test[1-8](_yesterday)?$') \n",
    "\n",
    "print(\"fitting started\")\n",
    "# Initialize your imputer\n",
    "imputer = ReMasker()\n",
    "\n",
    "# Since ReMasker is an imputer, we directly fit it on X_train\n",
    "imputer.fit(X_train)\n",
    "\n",
    "print(\"fitting finished\")\n",
    "\n",
    "# Function to mask values in a column\n",
    "def mask_values(data, column):\n",
    "    masked_data = data.copy()\n",
    "    # Randomly mask a certain percentage of the column - adjust as necessary\n",
    "    mask = np.random.rand(len(masked_data)) < 0.1\n",
    "    masked_data.loc[mask, column] = np.nan\n",
    "    return masked_data\n",
    "print(X_test.columns)\n",
    "# Evaluate performance for each of the testX columns\n",
    "for column, column_name in enumerate(X_test.columns):\n",
    "    print(\"we are in\")\n",
    "    # Create a copy of X_test with the current column masked\n",
    "    column=column\n",
    "    X_test_masked = mask_values(X_test, column_name)\n",
    "    \n",
    "    # Impute missing values\n",
    "    X_test_imputed =  pd.DataFrame(imputer.transform(X_test_masked).cpu().numpy())\n",
    "    \n",
    "    print(\"sucessfully imputed?\")\n",
    "\n",
    "    thingOne=X_test_imputed.iloc[:,column].dropna()\n",
    "\n",
    "    thingOne.to_csv(f\"{column_name}imputed.csv\")\n",
    "    thingTwo=X_test.iloc[:,column].dropna()\n",
    "    thingTwo.to_csv(f\"{column_name}.csv\")\n",
    "    # print(X_test.shape,X_test_imputed.shape)\n",
    "    # Calculate RMSE, MAE, and R2 for the imputed column\n",
    "    rmse = sqrt(mean_squared_error(X_test.iloc[:,column].dropna(), X_test_imputed.iloc[:,column].dropna()))\n",
    "    mae = mean_absolute_error(X_test.iloc[:,column].dropna(), X_test_imputed.iloc[:,column].dropna())\n",
    "    r2 = r2_score(X_test.iloc[:,column].dropna(), X_test_imputed.iloc[:,column].dropna())\n",
    "    \n",
    "    print(f\"Evaluation for {column_name}: RMSE = {rmse}, MAE = {mae}, R2 = {r2}\")\n",
    "with open('evaluation_results_yesterday.txt', 'w') as file:\n",
    "    for column, column_name in enumerate(X_test.columns[:8]):\n",
    "        # Create a copy of X_test with the current column masked\n",
    "        X_test_masked = X_test.copy()\n",
    "        X_test_masked.iloc[:,column]=np.nan\n",
    "        \n",
    "        # Impute missing values\n",
    "        X_test_imputed =  pd.DataFrame(imputer.transform(X_test_masked).cpu().numpy())\n",
    "        \n",
    "\n",
    "\n",
    "        # Calculate RMSE, MAE, and R2 for the imputed column\n",
    "        rmse = sqrt(mean_squared_error(X_test.iloc[:, column].dropna(), X_test_imputed.iloc[:, column].dropna()))\n",
    "        mae = mean_absolute_error(X_test.iloc[:, column].dropna(), X_test_imputed.iloc[:, column].dropna())\n",
    "        r2 = r2_score(X_test.iloc[:, column].dropna(), X_test_imputed.iloc[:, column].dropna())\n",
    "        \n",
    "        # Construct the output string\n",
    "        output_str = f\"Evaluation for {column_name}: RMSE = {rmse}, MAE = {mae}, R2 = {r2}\\n\"\n",
    "        \n",
    "        # Write to file and print\n",
    "        file.write(output_str)\n",
    "        print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b529769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'sagemaker-studio-905418013525-nvxe84zgs6'\n",
    "file_path = 'Labrado/alllabs1000adm/Labitemlist.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "Labitemlist = pd.read_csv(obj['Body'], header=None,names=['labitem', 'cnt'], dtype=int)\n",
    "\n",
    "file_path = 'Labrado/alllabs1000adm/unique_adm_year_day.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "unique_adm_year_day = pd.read_csv(obj['Body'], header=None,names=['hadm_id', 'chartyear', 'day','rowinx'], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1837ea9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labitemlist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd05cf3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>chartyear</th>\n",
       "      <th>day</th>\n",
       "      <th>rowinx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000019</td>\n",
       "      <td>2159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000019</td>\n",
       "      <td>2159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000019</td>\n",
       "      <td>2159</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20000024</td>\n",
       "      <td>2151</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20000034</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hadm_id  chartyear  day  rowinx\n",
       "0  20000019       2159    0       0\n",
       "1  20000019       2159    1       1\n",
       "2  20000019       2159    2       2\n",
       "3  20000024       2151    0       3\n",
       "4  20000034       2174    0       4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_adm_year_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6abede44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split year is 2178\n"
     ]
    }
   ],
   "source": [
    "unique_hadm_by_year = unique_adm_year_day.groupby('chartyear')['hadm_id'].nunique()\n",
    "cumulative_sum = unique_hadm_by_year.cumsum()\n",
    "total_sum = cumulative_sum.iloc[-1]\n",
    "split_index = cumulative_sum[cumulative_sum >= 0.8 * total_sum].index[0]\n",
    "print(\"split year is\",split_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c414cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Row_Train = unique_adm_year_day['chartyear']<=split_index\n",
    "Train_inx = Row_Train[Row_Train].index\n",
    "Row_Test = unique_adm_year_day['chartyear']>split_index\n",
    "Test_inx = Row_Test[Row_Test].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83274633-669d-4ab9-8b9b-15f50ca958f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([      0,       1,       2,       3,       4,       5,       6,       7,\n",
       "             8,       9,\n",
       "       ...\n",
       "       1966608, 1966609, 1966610, 1966611, 1966612, 1966613, 1966617, 1966618,\n",
       "       1966619, 1966620],\n",
       "      dtype='int64', length=1582939)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_inx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array = np.genfromtxt('filename.txt', delimiter=' ', usecols=range(10))-- to test later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "689312d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Labrado/alllabs1000adm/npval.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "npval = pd.read_csv(obj['Body'], header=None, dtype=float)\n",
    "npval_train = npval.iloc[Train_inx[:1000],:5]\n",
    "npval_test = npval.iloc[Test_inx[:1000],:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c88e132-07a8-4ba3-83e3-764d1ef63f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5</td>\n",
       "      <td>137.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.7</td>\n",
       "      <td>136.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.9</td>\n",
       "      <td>138.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.2</td>\n",
       "      <td>140.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>32.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.8</td>\n",
       "      <td>141.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2     3    4     5     6     7      8      9    ...  365  \\\n",
       "0  3.5  137.0  103.0  26.5  1.1  24.0  14.0  19.0  169.0  183.0  ...  NaN   \n",
       "1  3.7  136.0  100.0  28.1  1.2  28.0  12.0  17.0  190.0  195.0  ...  NaN   \n",
       "2  3.9  138.0  102.0  24.4  0.9  26.0  14.0  14.0  177.0  188.0  ...  NaN   \n",
       "3  5.2  140.0  104.0  32.1  1.1  25.0  16.0  28.0   88.0  196.0  ...  NaN   \n",
       "4  4.8  141.0  106.0  30.3  2.3  24.0  11.0  26.0  165.0  157.0  ...  NaN   \n",
       "\n",
       "   366  367  368  369  370  371  372  373  374  \n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 375 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'Labrado/alllabs1000adm/nptime.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "nptime = pd.read_csv(obj['Body'], header=None, dtype=float)\n",
    "nptime_train = nptime.iloc[Train_inx[:1000],:5]\n",
    "nptime_test = nptime.iloc[Test_inx[:1000],:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e598416a-7bdc-4453-bf70-e4a75589294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Labrado/alllabs1000adm/nptime.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "nptime = pd.read_csv(obj['Body'], header=None, dtype=float)\n",
    "nptime_train = nptime.iloc[Train_inx[:1000],:5]\n",
    "nptime_test = nptime.iloc[Test_inx[:1000],:5]\n",
    "del nptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5581fb89-af46-4589-b1df-433e7248541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Labrado/alllabs1000adm/npval_last.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "npval_last = pd.read_csv(obj['Body'], header=None, dtype=float)\n",
    "npval_last_train = npval_last.iloc[Train_inx[:1000],:5]\n",
    "npval_last_test = npval_last.iloc[Test_inx[:1000],:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62ed622e-b753-4894-9c28-ddecd9393f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  365  366  367  368  \\\n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN   \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN   \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN   \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN   \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   369  370  371  372  373  374  \n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 375 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npval_last.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8da9f3d1-82e2-4479-9349-b8a022d08401",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Labrado/alllabs1000adm/nptime_last.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "nptime_last = pd.read_csv(obj['Body'], header=None, dtype=float)\n",
    "nptime_last_train = nptime_last.iloc[Train_inx[:1000],:5]\n",
    "nptime_last_test = nptime_last.iloc[Test_inx[:1000],:5]\n",
    "del nptime_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0af8639f-36be-446d-a026-329dd9bc4c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2     3     4\n",
       "0     18.0  18.0  18.0  18.0  18.0\n",
       "1     18.0  18.0  18.0  18.0  18.0\n",
       "2     17.0  17.0  17.0  14.0  17.0\n",
       "3     17.0  17.0  17.0  17.0  17.0\n",
       "4     17.0  17.0  17.0  17.0  17.0\n",
       "...    ...   ...   ...   ...   ...\n",
       "1245  18.0  18.0  18.0  18.0  18.0\n",
       "1246  18.0  18.0  18.0  18.0  18.0\n",
       "1247   NaN   NaN   NaN  16.0   NaN\n",
       "1248  15.0  15.0  15.0  15.0  15.0\n",
       "1249   NaN   NaN   NaN  18.0  13.0\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nptime_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63df8449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "# current implementation: only support numerical values\n",
    "import numpy as np\n",
    "import torch, os\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "class MaskEmbed(nn.Module):\n",
    "    \"\"\" record to mask embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, rec_len=25, embed_dim=64, norm_layer=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.rec_len = rec_len\n",
    "        self.proj = nn.Conv1d(1, embed_dim, kernel_size=1, stride=1)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L = x.shape\n",
    "        # assert(L == self.rec_len, f\"Input data width ({L}) doesn't match model ({self.rec_len}).\")\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ActiveEmbed(nn.Module):\n",
    "    \"\"\" record to mask embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, rec_len=25, embed_dim=64, norm_layer=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.rec_len = rec_len\n",
    "        self.proj = nn.Conv1d(1, embed_dim, kernel_size=1, stride=1)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L = x.shape\n",
    "        # assert(L == self.rec_len, f\"Input data width ({L}) doesn't match model ({self.rec_len}).\")\n",
    "        x = self.proj(x)\n",
    "        x = torch.sin(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        #   x = torch.cat((torch.sin(x), torch.cos(x + math.pi/2)), -1)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, pos, cls_token=False):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = np.arange(pos)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    pos_embed = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr, min_lr, max_epochs, warmup_epochs):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        tmp_lr = lr * epoch / warmup_epochs \n",
    "    else:\n",
    "        tmp_lr = min_lr + (lr - min_lr) * 0.5 * \\\n",
    "            (1. + math.cos(math.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = tmp_lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = tmp_lr\n",
    "    return tmp_lr\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == np.inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "class NativeScaler:\n",
    "\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "class MAEDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, M):        \n",
    "         self.X = X\n",
    "         self.M = M\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.X[idx], self.M[idx]\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(dataset : str, path : str):\n",
    "\n",
    "    if dataset in ['climate', 'compression', 'wine', 'yacht', 'spam', 'letter', 'credit', 'raisin', 'bike', 'obesity', 'airfoil', 'blood', 'yeast', 'health', 'review', 'travel']:\n",
    "        df = pd.read_csv(os.path.join(path, 'data', dataset + '.csv'))\n",
    "        last_col = df.columns[-1]\n",
    "        y = df[last_col]\n",
    "        X = df.drop(columns=[last_col])\n",
    "    elif dataset == 'california':\n",
    "        from sklearn.datasets import fetch_california_housing\n",
    "        X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "    elif dataset == 'diabetes':\n",
    "        from sklearn.datasets import load_diabetes\n",
    "        X, y = load_diabetes(as_frame=True, return_X_y=True)\n",
    "    elif dataset == 'iris':\n",
    "        # only for testing\n",
    "        from sklearn.datasets import load_iris\n",
    "        X, y = load_iris(as_frame=True, return_X_y=True)\n",
    "        print(\"loaded iris dataset\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('MAE pre-training', add_help=False)\n",
    "    parser.add_argument('--dataset', default='california', type=str)\n",
    "    parser.add_argument('--batch_size', default=64, type=int,\n",
    "                        help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus')\n",
    "    parser.add_argument('--max_epochs', default=600, type=int)\n",
    "    parser.add_argument('--accum_iter', default=1, type=int,\n",
    "                        help='Accumulate gradient iterations (for increasing the effective batch size under memory constraints)')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--mask_ratio', default=0.5, type=float, help='Masking ratio (percentage of removed patches).')\n",
    "    parser.add_argument('--embed_dim', default=32, type=int, help='embedding dimensions')\n",
    "    parser.add_argument('--depth', default=6, type=int, help='encoder depth')\n",
    "    parser.add_argument('--decoder_depth', default=4, type=int, help='decoder depth')\n",
    "    parser.add_argument('--num_heads', default=4, type=int, help='number of heads')\n",
    "    parser.add_argument('--mlp_ratio', default=4., type=float, help='mlp ratio')\n",
    "    parser.add_argument('--encode_func', default='linear', type=str, help='encoding function')\n",
    "\n",
    "    parser.add_argument('--norm_field_loss', default=False,\n",
    "                        help='Use (per-patch) normalized field as targets for computing loss')\n",
    "    parser.set_defaults(norm_field_loss=False)\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05, help='weight decay (default: 0.05)')\n",
    "    parser.add_argument('--lr', type=float, default=None, metavar='LR', help='learning rate (absolute lr)')\n",
    "    parser.add_argument('--blr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='base learning rate: absolute_lr = base_lr * total_batch_size / 256')\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-5, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0')\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=40, metavar='N', help='epochs to warmup LR')\n",
    "\n",
    "    ###### change this path\n",
    "    parser.add_argument('--path', default='/data/tianyu/remasker/', type=str, help='dataset path')\n",
    "    parser.add_argument('--exp_name', default='test', type=str, help='experiment name')\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--device', default='cuda', help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=666, type=int)\n",
    "\n",
    "    parser.add_argument('--overwrite', default=True, help='whether to overwrite default config')\n",
    "    parser.add_argument('--pin_mem', action='store_false')\n",
    "\n",
    "    # distributed training parameters\n",
    "    return parser\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    X = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n",
    "    X = X.unsqueeze(1)\n",
    "    mask_embed = ActiveEmbed(4, 6)\n",
    "    print(mask_embed(X).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af63d0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.5094, grad_fn=<AddBackward0>), tensor([[[0.8098],\n",
      "         [0.8507],\n",
      "         [0.1312],\n",
      "         [0.1784]]], grad_fn=<SliceBackward0>), tensor([[0., 0., 0., 1.]]), tensor([[0., 1., 0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "# current implementation: only support numerical values\n",
    "\n",
    "from functools import partial\n",
    "from tkinter import E\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from timm.models.vision_transformer import Block\n",
    "from utils import MaskEmbed, get_1d_sincos_pos_embed, ActiveEmbed\n",
    "eps = 1e-6\n",
    "\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    \n",
    "    \"\"\" Masked Autoencoder with Transformer backbone\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rec_len=25, embed_dim=64, depth=4, num_heads=4,\n",
    "        decoder_embed_dim=64, decoder_depth=2, decoder_num_heads=4,\n",
    "        mlp_ratio=4., norm_layer=nn.LayerNorm, norm_field_loss=False, encode_func='linear'):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        \n",
    "        if encode_func == 'active':\n",
    "            self.mask_embed = ActiveEmbed(rec_len, embed_dim)\n",
    "        else:\n",
    "            self.mask_embed = MaskEmbed(rec_len, embed_dim)\n",
    "        \n",
    "        self.rec_len = rec_len\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, rec_len + 1, embed_dim), requires_grad=False)  \n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, rec_len + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, 1, bias=True)  # decoder to patch\n",
    "        \n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_field_loss = norm_field_loss\n",
    "        self.initialize_weights()\n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \n",
    "        # initialization\n",
    "        \n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.mask_embed.rec_len, cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_1d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], self.mask_embed.rec_len, cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.mask_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "    def random_masking(self, x, m, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        if self.training:\n",
    "            len_keep = int(L * (1 - mask_ratio))\n",
    "        else:\n",
    "            len_keep = int(torch.min(torch.sum(m, dim=1)))\n",
    "\n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        noise[m < eps] = 1\n",
    "\n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "        \n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "        nask = torch.ones([N, L], device=x.device) - mask\n",
    "\n",
    "        if self.training:\n",
    "            mask[m < eps] = 0\n",
    "\n",
    "        return x_masked, mask, nask, ids_restore\n",
    "\n",
    "\n",
    "    def forward_encoder(self, x, m, mask_ratio=0.5):\n",
    "        \n",
    "        # embed patches\n",
    "        x = self.mask_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]    \n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, nask, ids_restore = self.random_masking(x, m, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, nask, ids_restore\n",
    "\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        \n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        # x = self.decoder_pred(x)\n",
    "        x = torch.tanh(self.decoder_pred(x))/2 + 0.5\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward_loss(self, data, pred, mask, nask):\n",
    "        \"\"\"\n",
    "        data: [N, 1, L]\n",
    "        pred: [N, L]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        # target = self.patchify(data)\n",
    "        target = data.squeeze(dim=1)\n",
    "        if self.norm_field_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + eps)**.5\n",
    "        \n",
    "        \n",
    "        loss = (pred.squeeze(dim=2) - target) ** 2\n",
    "        loss = (loss * mask).sum() / mask.sum()  + (loss * nask).sum() / nask.sum()\n",
    "        # mean loss on removed patches\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def forward(self, data, miss_idx, mask_ratio=0.5):\n",
    "        \n",
    "        latent, mask, nask, ids_restore = self.forward_encoder(data, miss_idx, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore) \n",
    "        loss = self.forward_loss(data, pred, mask, nask)\n",
    "        return loss, pred, mask, nask\n",
    "\n",
    "\n",
    "def mae_base(**kwargs):\n",
    "    model = MaskedAutoencoder(\n",
    "        embed_dim=64, depth=8, num_heads=4,\n",
    "        decoder_embed_dim=64, decoder_depth=4, decoder_num_heads=4,\n",
    "        mlp_ratio=2., norm_layer=partial(nn.LayerNorm, eps=eps), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_medium(**kwargs):\n",
    "    model = MaskedAutoencoder(\n",
    "        embed_dim=32, depth=4, num_heads=4,\n",
    "        decoder_embed_dim=32, decoder_depth=4, decoder_num_heads=4,\n",
    "        mlp_ratio=4., norm_layer=partial(nn.LayerNorm, eps=eps), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_large(**kwargs):\n",
    "    model = MaskedAutoencoder(\n",
    "        embed_dim=64, depth=8, num_heads=4,\n",
    "        decoder_embed_dim=64, decoder_depth=4, decoder_num_heads=4,\n",
    "        mlp_ratio=4., norm_layer=partial(nn.LayerNorm, eps=eps), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    model = MaskedAutoencoder(\n",
    "        rec_len=4, embed_dim=8, depth=1, num_heads=1,\n",
    "        decoder_embed_dim=8, decoder_depth=1, decoder_num_heads=1,\n",
    "        mlp_ratio=4., norm_layer=partial(nn.LayerNorm, eps=eps)\n",
    "    )\n",
    "    \n",
    "    X = pd.DataFrame([[np.nan, 0.5, np.nan, 0.8]])\n",
    "\n",
    "    X = torch.tensor(X.values, dtype=torch.float32)\n",
    "    M = 1 - (1 * (np.isnan(X)))\n",
    "    X = torch.nan_to_num(X)\n",
    "    \n",
    "    X = X.unsqueeze(dim=1)\n",
    "    print(model.forward(X, M, 0.75))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d588d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from typing import Any, List, Tuple, Union\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import math, sys, argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "import time, os, json\n",
    "from utils import NativeScaler, MAEDataset, adjust_learning_rate, get_dataset\n",
    "import model_mae\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import sys\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "from utils import get_args_parser\n",
    "\n",
    "# hyperimpute absolute\n",
    "from hyperimpute.plugins.imputers import ImputerPlugin\n",
    "from sklearn.datasets import load_iris\n",
    "from hyperimpute.utils.benchmarks import compare_models\n",
    "from hyperimpute.plugins.imputers import Imputers\n",
    "from tqdm import tqdm\n",
    "eps = 1e-8\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ReMasker:\n",
    "\n",
    "    def __init__(self):\n",
    "        args = get_args_parser().parse_args()\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "        self.accum_iter = args.accum_iter\n",
    "        self.min_lr = args.min_lr\n",
    "        self.norm_field_loss = args.norm_field_loss\n",
    "        self.weight_decay = args.weight_decay\n",
    "        self.lr = args.lr\n",
    "        self.blr = args.blr\n",
    "        self.warmup_epochs = 20\n",
    "        self.model = None\n",
    "        self.norm_parameters = None\n",
    "\n",
    "        self.embed_dim = args.embed_dim\n",
    "        self.depth = args.depth\n",
    "        self.decoder_depth = args.decoder_depth\n",
    "        self.num_heads = args.num_heads\n",
    "        self.mlp_ratio = args.mlp_ratio\n",
    "        self.max_epochs = 50\n",
    "        self.mask_ratio = 0.5\n",
    "        self.encode_func = args.encode_func\n",
    "\n",
    "    def fit(self, X_raw: pd.DataFrame):\n",
    "\n",
    "        X = X_raw.copy()\n",
    "\n",
    "        # Parameters\n",
    "        no = len(X)\n",
    "        dim = X.shape[1]\n",
    "\n",
    "        # X = X.cpu()\n",
    "\n",
    "        min_val = np.zeros(dim)\n",
    "        max_val = np.zeros(dim)\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# Assuming X is a DataFrame and dim is the number of columns\n",
    "        dim = X.shape[1]\n",
    "        min_val = np.zeros(dim)\n",
    "        max_val = np.zeros(dim)\n",
    "        eps = 1e-7\n",
    "\n",
    "        for i in range(dim):\n",
    "            # Use .iloc to access the DataFrame by integer-location\n",
    "            min_val[i] = np.nanmin(X.iloc[:, i])\n",
    "            max_val[i] = np.nanmax(X.iloc[:, i])\n",
    "            # Perform the operation and update the column\n",
    "            X.iloc[:, i] = (X.iloc[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n",
    "\n",
    "        self.norm_parameters = {\"min\": min_val, \"max\": max_val}\n",
    "        np_array = X.to_numpy()\n",
    "\n",
    "        # Convert NumPy array to PyTorch tensor\n",
    "        X = torch.tensor(np_array, dtype=torch.float32)\n",
    "        # Set missing\n",
    "        M = 1 - (1 * (np.isnan(X)))\n",
    "        M = M.float().to(device)\n",
    "\n",
    "        X = torch.nan_to_num(X)\n",
    "        X = X.to(device)\n",
    "\n",
    "        self.model = model_mae.MaskedAutoencoder(\n",
    "            rec_len=dim,\n",
    "            embed_dim=self.embed_dim,\n",
    "            depth=self.depth,\n",
    "            num_heads=self.num_heads,\n",
    "            decoder_embed_dim=self.embed_dim,\n",
    "            decoder_depth=self.decoder_depth,\n",
    "            decoder_num_heads=self.num_heads,\n",
    "            mlp_ratio=self.mlp_ratio,\n",
    "            norm_layer=partial(nn.LayerNorm, eps=eps),\n",
    "            norm_field_loss=self.norm_field_loss,\n",
    "            encode_func=self.encode_func\n",
    "        )\n",
    "\n",
    "        # if self.improve and os.path.exists(self.path):\n",
    "        #     self.model.load_state_dict(torch.load(self.path))\n",
    "        #     self.model.to(device)\n",
    "        #     return self\n",
    "\n",
    "        self.model.to(device)\n",
    "\n",
    "        # set optimizers\n",
    "        # param_groups = optim_factory.add_weight_decay(model, args.weight_decay)\n",
    "        eff_batch_size = self.batch_size * self.accum_iter\n",
    "        if self.lr is None:  # only base_lr is specified\n",
    "            self.lr = self.blr * eff_batch_size / 64\n",
    "        # param_groups = optim_factory.add_weight_decay(self.model, self.weight_decay)\n",
    "        # optimizer = torch.optim.AdamW(param_groups, lr=self.lr, betas=(0.9, 0.95))\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, betas=(0.9, 0.95))\n",
    "        loss_scaler = NativeScaler()\n",
    "\n",
    "        dataset = MAEDataset(X, M)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, sampler=RandomSampler(dataset),\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        # if self.resume and os.path.exists(self.path):\n",
    "        #     self.model.load_state_dict(torch.load(self.path))\n",
    "        #     self.lr *= 0.5\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(self.max_epochs):\n",
    "            print(epoch)\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = 0\n",
    "\n",
    "            iter = 0\n",
    "\n",
    "            for iter, (samples, masks) in tqdm(enumerate(dataloader), total = len(dataloader)):\n",
    "\n",
    "                # we use a per iteration (instead of per epoch) lr scheduler\n",
    "                if iter % self.accum_iter == 0:\n",
    "                    adjust_learning_rate(optimizer, iter / len(dataloader) + epoch, self.lr, self.min_lr,\n",
    "                                         self.max_epochs, self.warmup_epochs)\n",
    "\n",
    "                samples = samples.unsqueeze(dim=1)\n",
    "                samples = samples.to(device, non_blocking=True)\n",
    "                masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "                # print(samples, masks)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss, _, _, _ = self.model(samples, masks, mask_ratio=self.mask_ratio)\n",
    "                    loss_value = loss.item()\n",
    "                    total_loss += loss_value\n",
    "\n",
    "                if not math.isfinite(loss_value):\n",
    "                    print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "                    sys.exit(1)\n",
    "\n",
    "                loss /= self.accum_iter\n",
    "                loss_scaler(loss, optimizer, parameters=self.model.parameters(),\n",
    "                            update_grad=(iter + 1) % self.accum_iter == 0)\n",
    "\n",
    "                if (iter + 1) % self.accum_iter == 0:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            total_loss = (total_loss / (iter + 1)) ** 0.5\n",
    "            # if total_loss < best_loss:\n",
    "            #     best_loss = total_loss\n",
    "            #     torch.save(self.model.state_dict(), self.path)\n",
    "            # if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            # print((epoch+1),',', total_loss)\n",
    "\n",
    "        # torch.save(self.model.state_dict(), self.path)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_raw: torch.Tensor):\n",
    "        if not torch.is_tensor(X_raw):\n",
    "            X_raw = torch.tensor(X_raw.values) \n",
    "        X = X_raw.clone()\n",
    "\n",
    "        min_val = self.norm_parameters[\"min\"]\n",
    "        max_val = self.norm_parameters[\"max\"]\n",
    "\n",
    "        no, dim = X.shape\n",
    "        X = X.cpu()\n",
    "\n",
    "        # MinMaxScaler normalization\n",
    "        for i in range(dim):\n",
    "            X[:, i] = (X[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n",
    "\n",
    "        # Set missing\n",
    "        M = 1 - (1 * (np.isnan(X)))\n",
    "        X = np.nan_to_num(X)\n",
    "\n",
    "        X = torch.from_numpy(X).to(device).float()\n",
    "        M = M.to(device).float()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Imputed data\n",
    "        with torch.no_grad():\n",
    "            for i in range(no):\n",
    "                sample = torch.reshape(X[i], (1, 1, -1))\n",
    "                mask = torch.reshape(M[i], (1, -1))\n",
    "                _, pred, _, _ = self.model(sample, mask)\n",
    "                pred = pred.squeeze(dim=2)\n",
    "                if i == 0:\n",
    "                    imputed_data = pred\n",
    "                else:\n",
    "                    imputed_data = torch.cat((imputed_data, pred), 0)\n",
    "\n",
    "                    # Renormalize\n",
    "        for i in range(dim):\n",
    "            imputed_data[:, i] = imputed_data[:, i] * (max_val[i] - min_val[i] + eps) + min_val[i]\n",
    "\n",
    "        if np.all(np.isnan(imputed_data.detach().cpu().numpy())):\n",
    "            err = \"The imputed result contains nan. This is a bug. Please report it on the issue tracker.\"\n",
    "            raise RuntimeError(err)\n",
    "\n",
    "        M = M.cpu()\n",
    "        imputed_data = imputed_data.detach().cpu()\n",
    "        # print('imputed', imputed_data, M)\n",
    "        # print('imputed', M * np.nan_to_num(X_raw.cpu()) + (1 - M) * imputed_data)\n",
    "        return M * np.nan_to_num(X_raw.cpu()) + (1 - M) * imputed_data\n",
    "\n",
    "    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Imputes the provided dataset using the GAIN strategy.\n",
    "        Args:\n",
    "            X: np.ndarray\n",
    "                A dataset with missing values.\n",
    "        Returns:\n",
    "            Xhat: The imputed dataset.\n",
    "        \"\"\"\n",
    "        X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        return self.fit(X).transform(X).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea5f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import get_args_parser\n",
    "from remasker_impute import ReMasker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "# X_raw = np.arange(50).reshape(10, 5) * 1.0\n",
    "# X = pd.DataFrame(X_raw, columns=['0', '1', '2', '3', '4'])\n",
    "# X.iat[3,0] = np.nan\n",
    "\n",
    "# imputer = ReMasker()\n",
    "# print(X)\n",
    "\n",
    "# imputed = imputer.fit_transform(X)\n",
    "# print(imputed)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923e1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'sagemaker-studio-905418013525-nvxe84zgs6'\n",
    "file_path = 'Labrado/alllabs1000adm/alllab1000adm.txt'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "rawdata = obj[\"Body\"].read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad235af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(sublist) for sublist in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac250110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>434243.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>450.466776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>896.759713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>206.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>479.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48506.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "count  434243.000000\n",
       "mean      450.466776\n",
       "std       896.759713\n",
       "min         0.000000\n",
       "25%        80.000000\n",
       "50%       206.000000\n",
       "75%       479.000000\n",
       "max     48506.000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e8daf61",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m data_lines:\n\u001b[1;32m      5\u001b[0m     words \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()  \u001b[38;5;66;03m# Splits by any whitespace\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     year \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m year\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2178\u001b[39m:\n\u001b[1;32m      8\u001b[0m         train\u001b[38;5;241m.\u001b[39mappend(words[\u001b[38;5;241m2\u001b[39m:])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data_lines = rawdata.split('\\n')\n",
    "train = []\n",
    "test = []\n",
    "# Iterate through each line and split by spaces\n",
    "for line in data_lines:\n",
    "    words = line.split()  # Splits by any whitespace\n",
    "    year = int(words[1])\n",
    "    if len(words)>0:\n",
    "        if year<=2178:\n",
    "            train.append(words[2:])\n",
    "        else:\n",
    "            test.append(words[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25b83689-e161-4049-bb2f-9e0b7a340030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['51222',\n",
       " '0',\n",
       " '9.2',\n",
       " '50983',\n",
       " '0',\n",
       " '137.0',\n",
       " '51249',\n",
       " '0',\n",
       " '34.8',\n",
       " '51277',\n",
       " '0',\n",
       " '12.6',\n",
       " '50912',\n",
       " '0',\n",
       " '1.1',\n",
       " '50902',\n",
       " '0',\n",
       " '103.0',\n",
       " '51006',\n",
       " '0',\n",
       " '19.0',\n",
       " '51301',\n",
       " '0',\n",
       " '15.7',\n",
       " '51221',\n",
       " '0',\n",
       " '26.5',\n",
       " '51248',\n",
       " '0',\n",
       " '29.8',\n",
       " '50960',\n",
       " '0',\n",
       " '1.5',\n",
       " '51265',\n",
       " '0',\n",
       " '183.0',\n",
       " '50868',\n",
       " '0',\n",
       " '14.0',\n",
       " '50971',\n",
       " '0',\n",
       " '3.5',\n",
       " '50882',\n",
       " '0',\n",
       " '24.0',\n",
       " '51250',\n",
       " '0',\n",
       " '86.0',\n",
       " '50931',\n",
       " '0',\n",
       " '169.0',\n",
       " '51279',\n",
       " '0',\n",
       " '3.09',\n",
       " '50893',\n",
       " '24',\n",
       " '8.8',\n",
       " '50902',\n",
       " '24',\n",
       " '100.0',\n",
       " '51222',\n",
       " '24',\n",
       " '9.7',\n",
       " '50868',\n",
       " '24',\n",
       " '12.0',\n",
       " '51277',\n",
       " '24',\n",
       " '12.4',\n",
       " '51301',\n",
       " '24',\n",
       " '8.9',\n",
       " '51250',\n",
       " '24',\n",
       " '86.0',\n",
       " '50983',\n",
       " '24',\n",
       " '136.0',\n",
       " '50971',\n",
       " '24',\n",
       " '3.7',\n",
       " '51006',\n",
       " '24',\n",
       " '17.0',\n",
       " '50931',\n",
       " '24',\n",
       " '190.0',\n",
       " '51248',\n",
       " '24',\n",
       " '29.8',\n",
       " '51249',\n",
       " '24',\n",
       " '34.7',\n",
       " '51221',\n",
       " '24',\n",
       " '28.1',\n",
       " '50912',\n",
       " '24',\n",
       " '1.2',\n",
       " '50960',\n",
       " '24',\n",
       " '2.1',\n",
       " '51279',\n",
       " '24',\n",
       " '3.27',\n",
       " '50970',\n",
       " '24',\n",
       " '3.1',\n",
       " '51265',\n",
       " '24',\n",
       " '195.0',\n",
       " '50882',\n",
       " '24',\n",
       " '28.0',\n",
       " '51277',\n",
       " '49',\n",
       " '12.5',\n",
       " '50868',\n",
       " '49',\n",
       " '14.0',\n",
       " '51301',\n",
       " '49',\n",
       " '5.8',\n",
       " '51222',\n",
       " '49',\n",
       " '8.4',\n",
       " '50983',\n",
       " '49',\n",
       " '138.0',\n",
       " '51265',\n",
       " '49',\n",
       " '183.0',\n",
       " '50960',\n",
       " '49',\n",
       " '1.7',\n",
       " '50912',\n",
       " '49',\n",
       " '0.9',\n",
       " '51250',\n",
       " '49',\n",
       " '86.0',\n",
       " '50971',\n",
       " '49',\n",
       " '3.9',\n",
       " '50931',\n",
       " '49',\n",
       " '177.0',\n",
       " '51006',\n",
       " '49',\n",
       " '14.0',\n",
       " '50882',\n",
       " '49',\n",
       " '26.0',\n",
       " '50902',\n",
       " '49',\n",
       " '102.0',\n",
       " '50970',\n",
       " '49',\n",
       " '2.7',\n",
       " '51221',\n",
       " '49',\n",
       " '23.9',\n",
       " '51279',\n",
       " '49',\n",
       " '2.79',\n",
       " '51249',\n",
       " '49',\n",
       " '34.9',\n",
       " '51248',\n",
       " '49',\n",
       " '29.9',\n",
       " '50893',\n",
       " '49',\n",
       " '8.9',\n",
       " '51248',\n",
       " '52',\n",
       " '29.5',\n",
       " '51277',\n",
       " '52',\n",
       " '12.3',\n",
       " '51222',\n",
       " '52',\n",
       " '8.3',\n",
       " '51249',\n",
       " '52',\n",
       " '34.1',\n",
       " '50924',\n",
       " '52',\n",
       " '333.0',\n",
       " '50953',\n",
       " '52',\n",
       " '215.0',\n",
       " '50954',\n",
       " '52',\n",
       " '125.0',\n",
       " '50952',\n",
       " '52',\n",
       " '21.0',\n",
       " '50935',\n",
       " '52',\n",
       " '224.0',\n",
       " '51301',\n",
       " '52',\n",
       " '6.1',\n",
       " '50998',\n",
       " '52',\n",
       " '165.0',\n",
       " '51265',\n",
       " '52',\n",
       " '188.0',\n",
       " '50885',\n",
       " '52',\n",
       " '0.4',\n",
       " '51283',\n",
       " '52',\n",
       " '2.3',\n",
       " '51221',\n",
       " '52',\n",
       " '24.4',\n",
       " '51250',\n",
       " '52',\n",
       " '87.0',\n",
       " '51279',\n",
       " '52',\n",
       " '2.82']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41296e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('testYesterdaydata.csv')\n",
    "train = pd.read_csv('trainYesterdayData.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Function to generate new column names\n",
    "def generate_new_name(old_name):\n",
    "    counter=1\n",
    "    mapping = {}\n",
    "    # Check if the column name is numeric or has a \"_yesterday\" suffix\n",
    "    base_name = old_name.replace('_yesterday', '')\n",
    "    if base_name.isdigit():\n",
    "        if base_name not in mapping:\n",
    "            # Assign a new testX name and increment the counter\n",
    "            mapping[base_name] = f'test{counter}'\n",
    "            counter += 1\n",
    "        # Return the mapped name, with \"_yesterday\" suffix if necessary\n",
    "        return mapping[base_name] + ('_yesterday' if '_yesterday' in old_name else '')\n",
    "    else:\n",
    "        # Non-numeric columns remain unchanged\n",
    "        return old_name\n",
    "\n",
    "# Apply the renaming function to each column\n",
    "test.columns = [generate_new_name(col) for col in test.columns]\n",
    "X_test = test.filter(regex='^test[1-8](_yesterday)?$') \n",
    "\n",
    "train.columns = [generate_new_name(col) for col in train.columns]\n",
    "X_train = train.filter(regex='^test[1-8](_yesterday)?$') \n",
    "\n",
    "print(\"fitting started\")\n",
    "# Initialize your imputer\n",
    "imputer = ReMasker()\n",
    "\n",
    "# Since ReMasker is an imputer, we directly fit it on X_train\n",
    "imputer.fit(X_train)\n",
    "\n",
    "print(\"fitting finished\")\n",
    "\n",
    "# Function to mask values in a column\n",
    "def mask_values(data, column):\n",
    "    masked_data = data.copy()\n",
    "    # Randomly mask a certain percentage of the column - adjust as necessary\n",
    "    mask = np.random.rand(len(masked_data)) < 0.1\n",
    "    masked_data.loc[mask, column] = np.nan\n",
    "    return masked_data\n",
    "print(X_test.columns)\n",
    "# Evaluate performance for each of the testX columns\n",
    "for column, column_name in enumerate(X_test.columns):\n",
    "    print(\"we are in\")\n",
    "    # Create a copy of X_test with the current column masked\n",
    "    column=column\n",
    "    X_test_masked = mask_values(X_test, column_name)\n",
    "    \n",
    "    # Impute missing values\n",
    "    X_test_imputed =  pd.DataFrame(imputer.transform(X_test_masked).cpu().numpy())\n",
    "    \n",
    "    print(\"sucessfully imputed?\")\n",
    "\n",
    "    thingOne=X_test_imputed.iloc[:,column].dropna()\n",
    "\n",
    "    thingOne.to_csv(f\"{column_name}imputed.csv\")\n",
    "    thingTwo=X_test.iloc[:,column].dropna()\n",
    "    thingTwo.to_csv(f\"{column_name}.csv\")\n",
    "    # print(X_test.shape,X_test_imputed.shape)\n",
    "    # Calculate RMSE, MAE, and R2 for the imputed column\n",
    "    rmse = sqrt(mean_squared_error(X_test.iloc[:,column].dropna(), X_test_imputed.iloc[:,column].dropna()))\n",
    "    mae = mean_absolute_error(X_test.iloc[:,column].dropna(), X_test_imputed.iloc[:,column].dropna())\n",
    "    r2 = r2_score(X_test.iloc[:,column].dropna(), X_test_imputed.iloc[:,column].dropna())\n",
    "    \n",
    "    print(f\"Evaluation for {column_name}: RMSE = {rmse}, MAE = {mae}, R2 = {r2}\")\n",
    "with open('evaluation_results_yesterday.txt', 'w') as file:\n",
    "    for column, column_name in enumerate(X_test.columns[:8]):\n",
    "        # Create a copy of X_test with the current column masked\n",
    "        X_test_masked = X_test.copy()\n",
    "        X_test_masked.iloc[:,column]=np.nan\n",
    "        \n",
    "        # Impute missing values\n",
    "        X_test_imputed =  pd.DataFrame(imputer.transform(X_test_masked).cpu().numpy())\n",
    "        \n",
    "\n",
    "\n",
    "        # Calculate RMSE, MAE, and R2 for the imputed column\n",
    "        rmse = sqrt(mean_squared_error(X_test.iloc[:, column].dropna(), X_test_imputed.iloc[:, column].dropna()))\n",
    "        mae = mean_absolute_error(X_test.iloc[:, column].dropna(), X_test_imputed.iloc[:, column].dropna())\n",
    "        r2 = r2_score(X_test.iloc[:, column].dropna(), X_test_imputed.iloc[:, column].dropna())\n",
    "        \n",
    "        # Construct the output string\n",
    "        output_str = f\"Evaluation for {column_name}: RMSE = {rmse}, MAE = {mae}, R2 = {r2}\\n\"\n",
    "        \n",
    "        # Write to file and print\n",
    "        file.write(output_str)\n",
    "        print(output_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
