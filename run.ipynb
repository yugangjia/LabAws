{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f15a9be-26d4-452c-909c-552927a2ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from typing import Any, List, Tuple, Union\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import math, sys, argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "import time, os, json\n",
    "from utils import NativeScaler, MAEDataset, adjust_learning_rate, get_dataset\n",
    "import MAE\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "from utils import get_args_parser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "from sklearn.datasets import load_iris\n",
    "from tqdm import tqdm\n",
    "eps = 1e-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
    "from math import sqrt\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ff91b-2006-4cfb-b2a0-71fc0fd05d50",
   "metadata": {},
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74af19be-0a88-499f-a4b1-bc165b27f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReMaskerStep:\n",
    "\n",
    "    def __init__(self, dim=16, mask_ratio=0.5, max_epochs=300, warmup_epochs=20, save_path=None, model=None, device=None, weigths=None, eps = 1e-7, normalize=True, nan=-1,\n",
    "                batch_size=64, accum_iter=1, min_lr=1e-5, norm_field_loss=False, \n",
    "                 weight_decay=0.05, lr=None, blr=1e-3, embed_dim=32, depth=6, \n",
    "                 decoder_depth=4, num_heads=4, mlp_ratio=4.0, encode_func='linear', **kwargs):\n",
    "        #args = get_args_parser().parse_args()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.accum_iter = accum_iter\n",
    "        self.min_lr = min_lr\n",
    "        self.norm_field_loss = norm_field_loss\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr = lr\n",
    "        self.blr = blr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.weigths = None\n",
    "        self.dim = dim\n",
    "        self.eps = 1e-7\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depth = depth\n",
    "        self.decoder_depth = decoder_depth\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.max_epochs = max_epochs\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.encode_func = encode_func\n",
    "        self.nan = nan\n",
    "        \n",
    "        if not save_path:\n",
    "            self.save_path = f'./checkpoints_{self.mask_ratio}'\n",
    "        else:\n",
    "            self.save_path = save_path\n",
    "            \n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "            \n",
    "        if not device:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        if not(model):\n",
    "            ### Model ###\n",
    "            self.model = MAE.MaskedAutoencoder(\n",
    "                rec_len=self.dim,\n",
    "                embed_dim=self.embed_dim,\n",
    "                depth=self.depth,\n",
    "                num_heads=self.num_heads,\n",
    "                decoder_embed_dim=self.embed_dim,\n",
    "                decoder_depth=self.decoder_depth,\n",
    "                decoder_num_heads=self.num_heads,\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                norm_layer=partial(nn.LayerNorm, eps=self.eps),\n",
    "                norm_field_loss=self.norm_field_loss,\n",
    "                encode_func=self.encode_func\n",
    "            )\n",
    "        else:\n",
    "            self.model = model\n",
    "            \n",
    "        \n",
    "        # Load Checkpoint if any\n",
    "        if weigths and os.path.exists(weigths):\n",
    "            self.model.load_state_dict(torch.load(weigths))\n",
    "            \n",
    "            \n",
    "        if torch.cuda.device_count() > 1:  # Checks for multiple GPUs\n",
    "            print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "            model = nn.DataParallel(model)\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        #self.normalize_vals = normalize\n",
    "        self.norm_parameters = None\n",
    "        \n",
    "\n",
    "    def calculate_norm_parameters(self, X: pd.DataFrame):\n",
    "        \n",
    "        min_val = np.zeros(self.dim)\n",
    "        max_val = np.zeros(self.dim)\n",
    "        \n",
    "        for i in range(self.dim):\n",
    "            # Use .iloc to access the DataFrame by integer-location\n",
    "            min_val[i] = np.nanmin(X.iloc[:, i])\n",
    "            max_val[i] = np.nanmax(X.iloc[:, i])\n",
    "        \n",
    "        self.norm_parameters = {\"min\": min_val, \"max\": max_val}\n",
    "        \n",
    "    def normalize(self, X_raw: pd.DataFrame, return_format='torch'):\n",
    "        X = X_raw.copy()\n",
    "        \n",
    "        if not(self.norm_parameters):\n",
    "            print('calculating norm parameters...')\n",
    "            self.calculate_norm_parameters(X)\n",
    "            \n",
    "        min_val = self.norm_parameters[\"min\"]\n",
    "        max_val = self.norm_parameters[\"max\"]\n",
    "\n",
    "        ### Normalization:\n",
    "        for i in range(self.dim):\n",
    "            # Perform the operation and update the column\n",
    "            X.iloc[:, i] = (X.iloc[:, i] - min_val[i]) / (max_val[i] - min_val[i] + self.eps)\n",
    "\n",
    "        self.norm_parameters = {\"min\": min_val, \"max\": max_val}\n",
    "        \n",
    "        if return_format == 'numpy':\n",
    "            np_array = X.to_numpy()\n",
    "            return np_array\n",
    "        elif return_format == 'torch': \n",
    "            np_array = X.to_numpy()\n",
    "            # Convert NumPy array to PyTorch tensor\n",
    "            X = torch.tensor(np_array, dtype=torch.float32)\n",
    "            return X\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "    def denormalize(self, imputed_data):\n",
    "    \n",
    "        min_val = self.norm_parameters[\"min\"]\n",
    "        max_val = self.norm_parameters[\"max\"]\n",
    "        \n",
    "        # Renormalize\n",
    "        for i in range(self.dim):\n",
    "            imputed_data[:, i] = imputed_data[:, i] * (max_val[i] - min_val[i] + self.eps) + min_val[i]\n",
    "            \n",
    "        return imputed_data\n",
    "        \n",
    "        \n",
    "\n",
    "    def fit(self, X_raw: pd.DataFrame, X_val=None, exclude_columns=None):\n",
    "        \n",
    "        #if self.normalize:\n",
    "        X = self.normalize(X_raw)\n",
    "            \n",
    "        # Set missing\n",
    "        M = 1 - (1 * (np.isnan(X)))\n",
    "        M = M.float().to(self.device)\n",
    "\n",
    "        X = torch.nan_to_num(X, nan=self.nan)\n",
    "        X = X.to(self.device)\n",
    "\n",
    "        # set optimizers\n",
    "        # param_groups = optim_factory.add_weight_decay(model, args.weight_decay)\n",
    "        eff_batch_size = self.batch_size * self.accum_iter\n",
    "        if self.lr is None:  # only base_lr is specified\n",
    "            self.lr = self.blr * eff_batch_size / 64\n",
    "            \n",
    "        # param_groups = optim_factory.add_weight_decay(self.model, self.weight_decay)\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, betas=(0.9, 0.95))\n",
    "        loss_scaler = NativeScaler()\n",
    "\n",
    "        dataset = MAEDataset(X, M)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, sampler=RandomSampler(dataset),\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        \n",
    "        # To store validation results\n",
    "        results_csv_path = os.path.join(self.save_path, 'validation_results.csv')\n",
    "        \n",
    "        ############ Train Loop ############\n",
    "        for epoch in range(self.max_epochs):\n",
    "            self.model.train()\n",
    "            print(epoch)\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = 0\n",
    "\n",
    "            iter = 0\n",
    "            eight = True\n",
    "\n",
    "            for iter, (samples, masks) in tqdm(enumerate(dataloader), total = len(dataloader)):\n",
    "                \n",
    "                # we use a per iteration (instead of per epoch) lr scheduler\n",
    "                if iter % self.accum_iter == 0:\n",
    "                    adjust_learning_rate(optimizer, iter / len(dataloader) + epoch, self.lr, self.min_lr,\n",
    "                                         self.max_epochs, self.warmup_epochs)\n",
    "                \n",
    "                \n",
    "                # Add 1 dimension and send to device\n",
    "                samples = samples.unsqueeze(dim=1)\n",
    "                samples = samples.to(self.device, non_blocking=True)\n",
    "                masks = masks.to(self.device, non_blocking=True)\n",
    "\n",
    "                # Calculate the loss\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss, _, _, _ = self.model(samples, masks, mask_ratio=self.mask_ratio, exclude_columns=exclude_columns)\n",
    "                    loss_value = loss.item()\n",
    "                    total_loss += loss_value\n",
    "\n",
    "                if not math.isfinite(loss_value):\n",
    "                    print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "                    sys.exit(1)\n",
    "                \n",
    "                loss /= self.accum_iter\n",
    "                \n",
    "                # Calculate the gradient and backpropagate\n",
    "                loss_scaler(loss, optimizer, parameters=self.model.parameters(),\n",
    "                            update_grad=(iter + 1) % self.accum_iter == 0)\n",
    "                \n",
    "                # Set gradients to 0 each accum_iter iterations\n",
    "                if (iter + 1) % self.accum_iter == 0:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            total_loss = (total_loss / (iter + 1)) ** 0.5\n",
    "            \n",
    "            \n",
    "            ############ Validation ############\n",
    "            self.model.eval()\n",
    "            eight_str = str(eight)\n",
    "            if epoch % 30 == 0 and X_val is not None and not X_val.empty:\n",
    "\n",
    "                # Get a subset of data\n",
    "                if epoch != (self.max_epochs-1):\n",
    "                    X_test = X_val[:10000]\n",
    "                else: \n",
    "                    X_test = X_val\n",
    "                    \n",
    "                epoch_validation_results = []\n",
    "                \n",
    "                # Evaluate each lab value:\n",
    "                for column, column_name in enumerate(X_test.columns):\n",
    "                    \n",
    "                    # Ignore the time columns\n",
    "                    if column in exclude_columns:\n",
    "                        continue  \n",
    "                    \n",
    "                    # Only evaluate if the column contains values\n",
    "                    X_test_real = X_test[X_test[column].notna()]\n",
    "                    \n",
    "                    if len(X_test_real) < 1:\n",
    "                        print(f'The sampling size of test with in column: {column}, is only {len(X_test_real)}')\n",
    "                        continue\n",
    "                    \n",
    "                    X_test_masked = X_test_real.copy()\n",
    "                    # Mask all values in that column with NaN\n",
    "                    X_test_masked.iloc[:,column]=np.nan\n",
    "\n",
    "                    # Impute the values:\n",
    "                    X_test_imputed =  pd.DataFrame(self.transform(X_test_masked).cpu().numpy())\n",
    "\n",
    "                    # Classify into normal abnormal\n",
    "                    #actual_classes = X_test.iloc[:, column].apply(classify_value, args=normal_ranges[column_name])\n",
    "                    #predicted_classes = X_test_imputed.iloc[:, column].apply(classify_value, args=normal_ranges[column_name])\n",
    "\n",
    "                    # Calculate the metrics:\n",
    "                    #cm = confusion_matrix(actual_classes, predicted_classes, labels=['under', 'within', 'over'])\n",
    "\n",
    "                    # Calculate RMSE, MAE, and R2\n",
    "                    rmse = sqrt(mean_squared_error(X_test.iloc[:, column].dropna(), X_test_imputed.iloc[:, column].dropna()))\n",
    "                    mae = mean_absolute_error(X_test.iloc[:, column].dropna(), X_test_imputed.iloc[:, column].dropna())\n",
    "                    r2 = r2_score(X_test.iloc[:, column].dropna(), X_test_imputed.iloc[:, column].dropna())\n",
    "\n",
    "                    # Construct the output string\n",
    "                    #output_str = f\"Epoch{epoch} Evaluation for {column_name}: RMSE = {rmse}, MAE = {mae}, R2 = {r2}, Confusion Matrix: {cm.tolist()}\\n\"\n",
    "                    output_str = f\"Epoch{epoch} Evaluation for {column_name}: RMSE = {rmse}, MAE = {mae}, R2 = {r2}\\n\"\n",
    "                    print(output_str)\n",
    "                    \n",
    "                    epoch_validation_results.append({\n",
    "                        'Epoch': epoch,\n",
    "                        'Column': column_name,\n",
    "                        'RMSE': rmse,\n",
    "                        'MAE': mae,\n",
    "                        'R2': r2\n",
    "                    })\n",
    "\n",
    "                results_df = pd.DataFrame(epoch_validation_results)\n",
    "\n",
    "                # Check if file exists to determine if we need to write headers\n",
    "                if not os.path.exists(results_csv_path):\n",
    "                    results_df.to_csv(results_csv_path, index=False)  # Include header\n",
    "                else:\n",
    "                    results_df.to_csv(results_csv_path, mode='a', header=False, index=False)  # Append without header\n",
    "\n",
    "          \n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print((epoch+1),',', total_loss)\n",
    "                # Check if dir exists, if not, create the dir\n",
    "                os.makedirs(self.save_path, exist_ok=True)\n",
    "                torch.save(self.model.state_dict(), f'{self.save_path}/epoch{epoch+1}_checkpoint')\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X_raw: pd.DataFrame, eval_batch_size=None):\n",
    "        \n",
    "        no = X_raw.shape[0]\n",
    "        \n",
    "        #if self.normalize:\n",
    "        X = self.normalize(X_raw)\n",
    "            \n",
    "        M = 1 - (1 * (np.isnan(X)))\n",
    "        M = M.float().to(self.device)\n",
    "        \n",
    "        X = torch.nan_to_num(X, nan=self.nan)\n",
    "        X = X.to(self.device)\n",
    "        \n",
    "        dataset = MAEDataset(X, M)\n",
    "        if eval_batch_size:\n",
    "            dataloader = DataLoader(\n",
    "                dataset, sampler=SequentialSampler(dataset),\n",
    "                batch_size=eval_batch_size, \n",
    "                drop_last=False\n",
    "            )\n",
    "        else:\n",
    "            dataloader = DataLoader(\n",
    "                dataset, sampler=SequentialSampler(dataset),\n",
    "                batch_size=self.batch_size, \n",
    "                drop_last=False\n",
    "            )\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Imputed data\n",
    "        imputed_data_list = []\n",
    "        with torch.no_grad():\n",
    "            for sample, mask in dataloader:\n",
    "                sample = sample.unsqueeze(1)\n",
    "                sample.to(self.device)\n",
    "                mask.to(self.device)\n",
    "                _, pred, _, _ = self.model(sample, mask)\n",
    "                pred = pred.squeeze(dim=2)\n",
    "                imputed_data_list.append(pred)\n",
    "\n",
    "        imputed_data = torch.cat(imputed_data_list, 0)\n",
    "        imputed_data = self.denormalize(imputed_data)\n",
    "\n",
    "\n",
    "        if np.all(np.isnan(imputed_data.detach().cpu().numpy())):\n",
    "            err = \"The imputed result contains nan. This is a bug. Please report it on the issue tracker.\"\n",
    "            raise RuntimeError(err)\n",
    "\n",
    "        M = M.cpu()\n",
    "        imputed_data = imputed_data.detach().cpu()\n",
    "        \n",
    "        if not torch.is_tensor(X_raw):\n",
    "            X_raw = torch.tensor(X_raw.values) \n",
    "\n",
    "        return M * np.nan_to_num(X_raw.cpu()) + (1 - M) * imputed_data\n",
    "\n",
    "    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Imputes the provided dataset using the GAIN strategy.\n",
    "        Args:\n",
    "            X: np.ndarray\n",
    "                A dataset with missing values.\n",
    "        Returns:\n",
    "            Xhat: The imputed dataset.\n",
    "        \"\"\"\n",
    "        X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        return self.fit(X).transform(X).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb91e5-ee95-4e9a-91ee-b6bfa83bdca8",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1e33c0-0c68-4239-a07e-657fcc8cb4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1582939, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>32.1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.8</td>\n",
       "      <td>17.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1      2     3      4     5     6     7    8     9   ...    30  \\\n",
       "0  3.5  18.0  137.0  18.0  103.0  18.0  26.5  18.0  1.1  18.0  ...   NaN   \n",
       "1  3.7  18.0  136.0  18.0  100.0  18.0  28.1  18.0  1.2  18.0  ...  24.0   \n",
       "2  3.9  17.0  138.0  17.0  102.0  17.0  24.4  14.0  0.9  17.0  ...  28.0   \n",
       "3  5.2  17.0  140.0  17.0  104.0  17.0  32.1  17.0  1.1  17.0  ...   NaN   \n",
       "4  4.8  17.0  141.0  17.0  106.0  17.0  30.3  17.0  2.3  17.0  ...   NaN   \n",
       "\n",
       "     31    32    33    34    35     36    37     38    39  \n",
       "0   NaN   NaN   NaN   NaN   NaN    NaN   NaN    NaN   NaN  \n",
       "1  42.0  14.0  42.0  19.0  42.0  169.0  42.0  183.0  42.0  \n",
       "2  42.0  12.0  42.0  17.0  42.0  190.0  42.0  195.0  42.0  \n",
       "3   NaN   NaN   NaN   NaN   NaN    NaN   NaN    NaN   NaN  \n",
       "4   NaN   NaN   NaN   NaN   NaN    NaN   NaN    NaN   NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/scratch/liyues_root/liyues/chenweiw/lab_values/10_labs.csv', header=None)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63221ae-2f86-4738-b8c5-89eda8db5020",
   "metadata": {},
   "source": [
    "### Split the df intro train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b577a6-b92d-4fdc-b9c1-9f051133e71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1266351, 40)\n",
      "Test shape: (316588, 40)\n",
      "Odd column indexes: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39]\n"
     ]
    }
   ],
   "source": [
    "# Split the dataframe into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)  # 20% of the data as test set\n",
    "\n",
    "# Print the shapes of the train and test dataframes\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# Create a list of odd column indexes\n",
    "time_column_indexes = [i for i in range(df.shape[1]) if i % 2 != 0]\n",
    "\n",
    "# Print the odd column indexes\n",
    "print(\"Odd column indexes:\", time_column_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a52a26e-08fd-4453-8acd-a722a47bf597",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb7720b-1fb3-443f-b552-d5f44574a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.shape[1]\n",
    "mask_ratio = 0.5\n",
    "max_epochs = 300\n",
    "save_path = '10_Labs_Train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1056117d-4f6e-47f2-9b4e-d5ad2ac8f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = ReMaskerStep(dim=columns, mask_ratio=mask_ratio, max_epochs=max_epochs, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86de46-bd18-4f7a-900b-4a22f2daf91f",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec04cc-e046-4450-8d07-e37ae9f02f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating norm parameters...\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:35<00:00, 50.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0 Evaluation for 0: RMSE = 0.83354032564747, MAE = 0.6551566178101119, R2 = -1.3645392779018364\n",
      "\n",
      "Epoch0 Evaluation for 2: RMSE = 11.066361753364026, MAE = 8.973921206874094, R2 = -5.813309195725753\n",
      "\n",
      "Epoch0 Evaluation for 4: RMSE = 9.015458321504338, MAE = 7.060514823205823, R2 = -1.835572227726325\n",
      "\n",
      "Epoch0 Evaluation for 6: RMSE = 10.037122861733545, MAE = 8.239876821650078, R2 = -1.567106171220817\n",
      "\n",
      "Epoch0 Evaluation for 8: RMSE = 1.5252524592524044, MAE = 0.7960259334486625, R2 = -0.0559097376409341\n",
      "\n",
      "Epoch0 Evaluation for 10: RMSE = 5.044840142172688, MAE = 3.8950325778029473, R2 = -0.37941840295138984\n",
      "\n",
      "Epoch0 Evaluation for 12: RMSE = 3.856253420083287, MAE = 2.951699946240759, R2 = -0.26584314717289304\n",
      "\n",
      "Epoch0 Evaluation for 14: RMSE = 19.784864575566676, MAE = 12.96078433981272, R2 = -0.02099831796584728\n",
      "\n",
      "Epoch0 Evaluation for 16: RMSE = 61.76746030012727, MAE = 40.27490646904438, R2 = -0.3062642741601356\n",
      "\n",
      "Epoch0 Evaluation for 18: RMSE = 145.81379241573043, MAE = 114.10491720024362, R2 = -0.1965667222855496\n",
      "\n",
      "Epoch0 Evaluation for 20: RMSE = 0.7719129414887558, MAE = 0.5920315438320217, R2 = -0.9434032195974122\n",
      "\n",
      "Epoch0 Evaluation for 22: RMSE = 6.9684150697594935, MAE = 5.639743261908678, R2 = -1.615244009601784\n",
      "\n",
      "Epoch0 Evaluation for 24: RMSE = 8.517201091735837, MAE = 6.777031059317179, R2 = -1.4245316878526024\n",
      "\n",
      "Epoch0 Evaluation for 26: RMSE = 9.573753855234761, MAE = 7.476709253644753, R2 = -1.1218416703599998\n",
      "\n",
      "Epoch0 Evaluation for 28: RMSE = 1.5035742394447993, MAE = 0.8629146448547307, R2 = -0.09244457554393448\n",
      "\n",
      "Epoch0 Evaluation for 30: RMSE = 5.7832763204884055, MAE = 4.546317059718332, R2 = -0.8381669689540554\n",
      "\n",
      "Epoch0 Evaluation for 32: RMSE = 4.181134889536529, MAE = 3.196519290254635, R2 = -0.5846623391589534\n",
      "\n",
      "Epoch0 Evaluation for 34: RMSE = 19.94303292815411, MAE = 13.298290654952572, R2 = -0.03297460839524802\n",
      "\n",
      "Epoch0 Evaluation for 36: RMSE = 62.065650638971746, MAE = 42.36268956642664, R2 = -0.4989442032220148\n",
      "\n",
      "Epoch0 Evaluation for 38: RMSE = 150.03010231288363, MAE = 107.02511944615301, R2 = -0.21382451389341228\n",
      "\n",
      "1 , 0.504193880028449\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:20<00:00, 51.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:21<00:00, 51.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:20<00:00, 51.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:14<00:00, 52.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:17<00:00, 52.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:21<00:00, 51.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 , 0.4672753982352495\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 , 0.46725921683585364\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19787/19787 [06:02<00:00, 54.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 4666/19787 [01:25<04:36, 54.60it/s]"
     ]
    }
   ],
   "source": [
    "imputer.fit(train_df, test_df, exclude_columns=time_column_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6540fb49-2425-46a9-b33e-ccb951d79a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2ee50-5f78-4d41-bcd4-ea350dce3205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
